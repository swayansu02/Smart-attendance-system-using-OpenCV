{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "af4f5a8b",
      "metadata": {
        "id": "af4f5a8b"
      },
      "source": [
        "# <h1><center>Smart Attendance System using OpenCV - Image capture and model training</center></h1>\n",
        "<center>By -  Swayansu Mishra  </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5772df22",
      "metadata": {
        "id": "5772df22"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af9c50c7",
      "metadata": {
        "id": "af9c50c7"
      },
      "source": [
        "This project is aimed at developing a Smart Attendance System using OpenCV and a Convolutional Neural Network (CNN). The system captures images of people, trains a CNN model to recognize their faces, and then uses the trained model to mark attendance in real-time by recognizing the person's face from the webcam feed.\n",
        "\n",
        "OpenCV is a powerful open-source library for computer vision, and it provides several tools for detecting and recognizing objects, including faces. In this project, we will use the Haar Cascade classifier to detect faces in the video frames. Haar Cascade classifiers are machine learning object detection methods that work on the principle of features proposed by Paul Viola and Michael Jones in their paper, \"Rapid Object Detection using a Boosted Cascade of Simple Features.\" These classifiers are trained to detect objects in images by analyzing the intensity differences between the adjacent rectangular regions.\n",
        "\n",
        "`haarcascade_frontalface_default.xml` is a pre-trained Haar Cascade classifier provided by OpenCV for detecting frontal faces in images. We will use this classifier to detect faces in the video frames captured by the webcam.\n",
        "\n",
        "The system consists of several key steps:\n",
        "\n",
        "1. Capturing images of individuals for training purposes\n",
        "2. Preprocessing the images and creating labels\n",
        "3. Splitting the dataset into training and testing sets\n",
        "4. Defining, compiling, and training a Convolutional Neural Network (CNN) model\n",
        "5. Marking attendance for recognized individuals in a CSV file\n",
        "6. Implementing a real-time face recognition system using the trained model\n",
        "7. Throughout this notebook, we provide detailed explanations, comments, and code sections to guide you through the development process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b21816d5",
      "metadata": {
        "id": "b21816d5"
      },
      "source": [
        "In this notebook we will focus on the steps from 1 to 4."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4c131cf",
      "metadata": {
        "id": "a4c131cf"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be4dd21b",
      "metadata": {
        "id": "be4dd21b"
      },
      "source": [
        "First, we need to import the necessary libraries for the project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26dae4a1",
      "metadata": {
        "id": "26dae4a1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, Dropout, Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "from sklearn.utils import shuffle\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afb31460",
      "metadata": {
        "id": "afb31460"
      },
      "source": [
        "## Capture Images for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd4a2730",
      "metadata": {
        "id": "dd4a2730"
      },
      "source": [
        "We will create functions to capture images of multiple people using a webcam. These images will be used to train our CNN model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92d697a0",
      "metadata": {
        "id": "92d697a0"
      },
      "source": [
        "### Function to capture images for a single person"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41508f74",
      "metadata": {
        "id": "41508f74"
      },
      "source": [
        "This function captures images of a single person using a webcam. The user can press 'c' to capture an image and 'q' to stop capturing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc1709b3",
      "metadata": {
        "id": "dc1709b3"
      },
      "outputs": [],
      "source": [
        "def capture_images_for_person(person_name, output_dir):\n",
        "    # Initialize the webcam for capturing video frames\n",
        "    cap = cv2.VideoCapture('http://172.16.1.169:4747/video')\n",
        "    cv2.namedWindow('Face Capture')\n",
        "    \n",
        "    # Get the current count of images for the person in the output directory\n",
        "    count = 0\n",
        "    for file in os.listdir(output_dir):\n",
        "        if file.startswith(person_name):\n",
        "            count += 1\n",
        "\n",
        "    # Loop to capture multiple images of a person\n",
        "    while True:\n",
        "        # Read frames from the webcam\n",
        "        ret, frame = cap.read()\n",
        "        # Display the captured frames\n",
        "        cv2.imshow('Face Capture', frame)\n",
        "\n",
        "        # Wait for the user to press 'c' or 'q'\n",
        "        key = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "        # If the user presses 'c', save the current frame as an image\n",
        "        if key == ord('c'):\n",
        "            count += 1\n",
        "            file_name = f\"{person_name}_{count}.jpg\"\n",
        "            file_path = os.path.join(output_dir, file_name)\n",
        "            cv2.imwrite(file_path, frame)\n",
        "            print(f\"{file_name} saved.\")\n",
        "\n",
        "        # If the user presses 'q', stop capturing images\n",
        "        if key == ord('q'):\n",
        "            break\n",
        "\n",
        "    # Release the webcam and close the windows\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "870500f1",
      "metadata": {
        "id": "870500f1"
      },
      "source": [
        "### Function to capture images for multiple people"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c38da6eb",
      "metadata": {
        "id": "c38da6eb"
      },
      "source": [
        "This function captures images for multiple people by asking the user for the number of people and their names. It then calls the capture_images_for_person() function for each person."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d18a392b",
      "metadata": {
        "id": "d18a392b"
      },
      "outputs": [],
      "source": [
        "def capture_images_for_multiple_people():\n",
        "    # Input the number of people to capture images for\n",
        "    num_people = int(input(\"Enter the number of people: \"))\n",
        "    output_dir = 'training_images'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Loop to capture images for each person\n",
        "    for i in range(num_people):\n",
        "        person_name = input(f\"Enter the name of person {i + 1}: \")\n",
        "        print(f\"Capturing images for {person_name}. Press 'c' to capture and 'q' to move on to the next person.\")\n",
        "        capture_images_for_person(person_name, output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62a7568d",
      "metadata": {
        "id": "62a7568d"
      },
      "source": [
        "Now, we will call the function to capture images using our webcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c8547d1",
      "metadata": {
        "id": "5c8547d1"
      },
      "outputs": [],
      "source": [
        "capture_images_for_multiple_people()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a9326ed",
      "metadata": {
        "id": "4a9326ed"
      },
      "source": [
        "## Train the CNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f528a4b",
      "metadata": {
        "id": "7f528a4b"
      },
      "source": [
        "We will now create a function to train a CNN model using the captured images. The model will be used to recognize faces in real-time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "384f00bf",
      "metadata": {
        "id": "384f00bf"
      },
      "source": [
        "### Preprocess images and labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8517dd9a",
      "metadata": {
        "id": "8517dd9a"
      },
      "source": [
        "In this step, we load and preprocess the images and labels from the 'training_images' folder. The images are resized to 96x96 pixels and converted to grayscale, while the labels are one-hot encoded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "858611eb",
      "metadata": {
        "id": "858611eb"
      },
      "outputs": [],
      "source": [
        "def preprocess_images_and_labels():\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    # Loop through the images and load them into a list\n",
        "    for idx, filename in enumerate(os.listdir('training_images')):\n",
        "        img = cv2.imread(os.path.join('training_images', filename), cv2.IMREAD_GRAYSCALE)\n",
        "        img = cv2.resize(img, (96, 96))  # Resize to match the input size of the CNN\n",
        "        images.append(img)\n",
        "        labels.append(filename.split('_')[0])\n",
        "\n",
        "    # Normalize the images\n",
        "    images = np.array(images).reshape(-1, 96, 96, 1).astype('float32') / 255.0\n",
        "\n",
        "    # Encode labels\n",
        "    encoder = LabelEncoder()\n",
        "    encoded_labels = encoder.fit_transform(labels)\n",
        "    one_hot_labels = np_utils.to_categorical(encoded_labels)\n",
        "\n",
        "    return images, one_hot_labels, encoder\n",
        "\n",
        "# Calling the function to preprocess the images and labels\n",
        "images, one_hot_labels, encoder = preprocess_images_and_labels()\n",
        "\n",
        "# Save the encoder\n",
        "with open('label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(encoder, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81085555",
      "metadata": {
        "id": "81085555"
      },
      "source": [
        "### Define, compile, and train the CNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d13ee9b",
      "metadata": {
        "id": "9d13ee9b"
      },
      "source": [
        "We define a simple CNN architecture with two convolutional layers, max-pooling layers, a fully connected layer, dropout, and an output layer. The model is compiled using the Adam optimizer and categorical cross-entropy loss function. Finally, we train the model using the preprocessed images and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11838a3a",
      "metadata": {
        "id": "11838a3a"
      },
      "outputs": [],
      "source": [
        "def create_custom_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        \n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        \n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        \n",
        "        Conv2D(256, (3, 3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        \n",
        "        Conv2D(512, (3, 3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        \n",
        "        GlobalAveragePooling2D(),\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def train_model(images, one_hot_labels, encoder):\n",
        "    # Shuffle the dataset\n",
        "    images, one_hot_labels = shuffle(images, one_hot_labels, random_state=42)\n",
        "    \n",
        "    # Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(images, one_hot_labels, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Make sure images have 3 channels\n",
        "    X_train_3_channels = np.repeat(X_train, 3, axis=-1)\n",
        "    X_test_3_channels = np.repeat(X_test, 3, axis=-1)\n",
        "\n",
        "    # Create the custom CNN model\n",
        "    base_model = create_custom_model(input_shape=(96, 96, 3))\n",
        "\n",
        "    # Add custom top layers for face recognition\n",
        "    x = base_model.output\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    predictions = Dense(len(encoder.classes_), activation='softmax')(x)\n",
        "\n",
        "    # Create the final model\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    # Define the data augmentation generator\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # Fit the generator on the training images\n",
        "    datagen.fit(X_train_3_channels)\n",
        "\n",
        "    # Train the model with data augmentation\n",
        "    model.fit(datagen.flow(X_train_3_channels, y_train, batch_size=32),\n",
        "              validation_data=(X_test_3_channels, y_test),\n",
        "              steps_per_epoch=len(X_train_3_channels) / 32,\n",
        "              epochs=130)\n",
        "    \n",
        "    # Save the model\n",
        "    model.save('face_recognition_model.h5')\n",
        "\n",
        "    return model\n",
        "\n",
        "model = train_model(images, one_hot_labels, encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c49e4df",
      "metadata": {
        "id": "5c49e4df"
      },
      "source": [
        "Next we need to focus on definining a function to mark the attendance and then face detection using Haar Cascade and finally using our saved model recgnizing the face and marking the attendance.\n",
        "This all will be covered in the next notebook."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}